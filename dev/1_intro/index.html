<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Theory and Notation · HMMGradients</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="HMMGradients logo"/></a><div class="docs-package-name"><span class="docs-autofit">HMMGradients</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li class="is-active"><a class="tocitem" href>Theory and Notation</a><ul class="internal"><li><a class="tocitem" href="#Hidden-Markov-models"><span>Hidden Markov models</span></a></li><li><a class="tocitem" href="#Maximum-likelihood-(ML)"><span>Maximum likelihood (ML)</span></a></li><li><a class="tocitem" href="#Scaled-probabilities"><span>Scaled probabilities</span></a></li><li><a class="tocitem" href="#Log-probabilities"><span>Log probabilities</span></a></li><li><a class="tocitem" href="#references"><span>References</span></a></li></ul></li><li><a class="tocitem" href="../2_fb/">Forward-Backward functions</a></li><li><a class="tocitem" href="../3_grads/">Computing gradients</a></li><li><a class="tocitem" href="../4_demo/">Demo</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Theory and Notation</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Theory and Notation</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/idiap/HMMGradients.jl/blob/master/docs/src/1_intro.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="theory"><a class="docs-heading-anchor" href="#theory">Theory and Notation</a><a id="theory-1"></a><a class="docs-heading-anchor-permalink" href="#theory" title="Permalink"></a></h1><h2 id="Hidden-Markov-models"><a class="docs-heading-anchor" href="#Hidden-Markov-models">Hidden Markov models</a><a id="Hidden-Markov-models-1"></a><a class="docs-heading-anchor-permalink" href="#Hidden-Markov-models" title="Permalink"></a></h2><p>A hidden Markov model (HMM)  with <span>$N_s$</span> states is defined by the following parameters:</p><ul><li><p><strong>Initial state probability vector</strong>: <span>$\mathbf{a} = [a_{1},\dots,a_{N_s}]^\intercal$</span> where the <span>$j$</span>-th element gives  the initial probability of being at state  <span>$s_t = j$</span> at time <span>$t=0$</span>.  Summing the elements of the vector gives 1.</p></li><li><p><strong>Transition matrix</strong>: <span>$\mathbf{A} \in \mathbb{R}^{N_s \times N_s}$</span> where the element <span>$a_{j,i}=p(s_t=j|s_{t-1}=i)$</span>  indicates the probability of going from state <span>$s_{t-1} = i$</span> to  state <span>$s_t = j$</span>. Requires to satisfy <span>$\sum_{j=1}^{N_s} a_{j,i} = 1$</span> for all  <span>$j= 1 \dots N_s$</span> i.e. the elements of the rows of <span>$\mathbf{A}$</span> must sum to 1.</p></li><li><p><strong>Observation probabilities</strong>: each state <span>$j=1,\dots,N_s$</span> is associated  for example with a probability density function <span>$b_i(\mathbf{x}_t)$</span>,  where <span>$\mathbf{x}_t \in \mathbb{R}^{N_x}$</span> is  a vector of observations at time <span>$t$</span>. For a sequence of <span>$N_t$</span> observations,  <span>$\mathbf{X} = [\mathbf{x}_1 \dots \mathbf{x}_{N_t}]^\intercal$</span>, the operator  <span>$B_{\mathcal{W}}: \mathbb{R}^{N_t \times N_x} \rightarrow \mathbb{R}^{N_t \times N_s}$</span> where <span>$\mathcal{W}$</span> is a set of parameters, maps the observations to <span>$\mathbf{Y} = [\mathbf{y}_1 \dots \mathbf{y}_{N_t}]^\intercal$</span>, where the element <span>$y_{t,j}=p(\mathbf{x}_t|s_t=j)$</span>  is the likelihood of <span>$\mathbf{x}_t$</span>  for a given state <span>$j$</span>.</p></li></ul><h2 id="Maximum-likelihood-(ML)"><a class="docs-heading-anchor" href="#Maximum-likelihood-(ML)">Maximum likelihood (ML)</a><a id="Maximum-likelihood-(ML)-1"></a><a class="docs-heading-anchor-permalink" href="#Maximum-likelihood-(ML)" title="Permalink"></a></h2><p>Given the observation <span>$\mathbf{X}$</span>, we would like to obtain a HMM  capable of describing this as sequences of states that are directly measurable. Typically a set of <span>$N_d$</span> observations  <span>$\mathcal{D} = \{ \mathbf{X}_1, \dots, \mathbf{X}_{N_d} \}$</span> is used but we consider only a single one  to simplify the notation. The optimal HMM parameters <span>$\mathcal{L}^\star =  \{ \mathbf{a}^\star, \mathbf{A}^\star, \mathcal{W}^\star \}$</span> can be obtained by maximizing the likelihood of the observations. This can be achieved by minimizing the  negative logarithm of this likelihood:</p><p class="math-container">\[\text{minimize}_{\mathcal{L}} 
\{ 
-\log( p (\mathbf{X}) ) = 
-\log( \sum_{ \mathbf{s} \in \mathcal{N} } 
p ( \mathbf{X} | \mathbf{s}  ) p ( \mathbf{s} ) 
)
\},\]</p><p>where <span>$\mathbf{s} \in \mathbb{N}^{N_t}$</span> is sequence of states,  and <span>$\mathcal{N}$</span> is the set of  all possible state sequences. Since <span>$\mathcal{N}$</span> has <span>$N_s^{N_t}$</span> elements, in general it is not feasible to compute the cost function by explicitly computing this summation. Instead, it is convenient to &quot;split&quot; <span>$p(\mathbf{X})$</span> at a particular time <span>$t$</span>:</p><p class="math-container">\[p(\mathbf{X}) = 
\sum_{i \in \mathcal{N}_{t,j}}
p(\mathbf{X},s_t=j),\]</p><p>where <span>$\mathcal{N}_{t,j}$</span> are the  allowed transitions from state <span>$j$</span> at time <span>$t$</span>,</p><p class="math-container">\[p(\mathbf{X},s_t=j) = 
p(\mathbf{x}_1,\dots,\mathbf{x}_t,s_t=j)
p(\mathbf{x}_{t+1},\dots,\mathbf{x}_{N_t} | \mathbf{x}_1,\dots,\mathbf{x}_t,s_t=j),\]</p><p>since we are assuming <em>observation independence</em>,  we can drop the conditions of the second term:</p><p class="math-container">\[p(\mathbf{x}_1,\dots,\mathbf{x}_t,s_t=j)
p(\mathbf{x}_{t+1},\dots,\mathbf{x}_{N_t},s_t=j | s_t=j) =
\alpha_{t,j} \beta_{t,j},\]</p><p>where <span>$\alpha_{t,j}$</span> and <span>$\beta_{t,j}$</span> are the  forward and backward probabilities. The computation of these probabilities can be performed in  a recursive way going either forward and backward in time.</p><p>The forward probabilities can be written as:</p><p class="math-container">\[\alpha_{1,j} = a_j y_{1,j}
\ \ \text{for} \ \ j =1,\dots,N_s\]</p><p class="math-container">\[\alpha_{t,j} = \sum_{i \in \mathcal{N}_{t,j}} \alpha_{t-1,i} 
a_{i,j} y_{t,j} 
\ \ \text{for} \ \ t = 2,\dots,N_t \ \ j=1,\dots,N_s,\]</p><p>and using matrix notation:</p><p class="math-container">\[\boldsymbol\alpha_1 = \text{diag}(\mathbf{y}_1) \mathbf{a}\]</p><p class="math-container">\[\boldsymbol\alpha_{t} =\text{diag}(\mathbf{y}_{t}) \mathbf{A}_t^\intercal  \boldsymbol\alpha_{t-1}
\ \ \text{for} \ \ t = 2,\dots,N_t \ \ j=1,\dots,N_s,\]</p><p>where <span>$\boldsymbol\alpha_t$</span> is a <span>$N_s$</span>-long vector containing the  forward probabilities at time <span>$t$</span>  and <span>$\mathbf{A}_t = \mathbf{N}_t \mathbf{A}$</span>, where <span>$\mathbf{N}_t$</span> is a selection matrix which includes  only the allowed indices at time <span>$t$</span>.</p><p>Backward probabilities:</p><p class="math-container">\[\beta_{N_t,j} = 1
\ \ \text{for} \ \  j =1,\dots,N_s\]</p><p class="math-container">\[\beta_{t,j} = \sum_{k \in \bar{\mathcal{N}}_{t+1,j}} \beta_{t+1,k} 
a_{j,k} y_{t+1,k}
\ \ \text{for} \ \ t = N_t-1,\dots,1\]</p><p>where <span>$\bar{\mathcal{N}}_{t,j}$</span> are the  allowed transitions arriving from state <span>$j$</span> at time <span>$t$</span>. Using matrix notation:</p><p class="math-container">\[\boldsymbol\beta_{N_t} = \mathbf{1}\]</p><p class="math-container">\[\boldsymbol\beta_{t} = \mathbf{A}_{t+1} \text{diag}(\mathbf{y}_{t+1})  \boldsymbol\beta_{t+1}
\ \ \text{for} \ \ t = N_t-1,\dots,1\]</p><p>where <span>$\boldsymbol\beta_t$</span> is a <span>$N_s$</span>-long vector containing the  backward probabilities at time <span>$t$</span>.  Finally the optimization problem can be written as:</p><p class="math-container">\[\text{minimize}_{\mathcal{L}} 
\{ 
-\log( p (\mathbf{X}) ) = -\log(\boldsymbol\alpha_{t}^\intercal \boldsymbol\beta_{t} )
\}
\ \text{s.t.} \ \mathbf{Y} = B_{\mathcal{W}}(\mathbf{X})\]</p><p>The classic way of solving this problem is Baum-Weltch, however this approach is has the limitation that <span>$B_{\mathcal{W}}$</span> must satisfy certain properties <a href="#references">[1]</a>. Another approach which enables the use of  deep neural networks is the gradient descent and all the other  gradient based methods. These methods requires the derivatives  with respect to <span>$\mathbf{a}$</span>, <span>$\mathbf{A}$</span> and <span>$\mathbf{Y}$</span> with the latter which is used to backpropagate through <span>$B_{\mathcal{W}}$</span>. As we will see the derivatives can be expressed  in terms of posterior probabilities,  i.e. the probability of being at state <span>$j$</span> given an observation at time <span>$t$</span>:</p><p class="math-container">\[\gamma_{t,j} = p(s_t=j|\mathbf{x}_t) = \frac{\alpha_{t,j} \beta_{t,j}}{p(\mathbf{X})}.\]</p><p>Unfortunately forward probabilities suffer of underflow as <span>$N_t$</span>  increases. To prevent this issue two techniques can be used:  either <span>$\boldsymbol\alpha_{t}$</span> can be scaled at each time step <span>$t$</span>  or the computation can be performed in the log domain. </p><h2 id="Scaled-probabilities"><a class="docs-heading-anchor" href="#Scaled-probabilities">Scaled probabilities</a><a id="Scaled-probabilities-1"></a><a class="docs-heading-anchor-permalink" href="#Scaled-probabilities" title="Permalink"></a></h2><p>In order to keep <span>$\boldsymbol\alpha_{t}$</span> in a good numerical range, forward probabilities can be normalized at every time step <a href="#references">[2-3]</a>:</p><p class="math-container">\[\bar{\alpha}_{t,j} = \prod_{\tau = 1}^t \frac{1}{c_\tau} \alpha_{t,j}, \]</p><p>where <span>$c_t$</span> is a normalization coefficient  that ensures that <span>$\sum_{j=1}^{N_s} \bar{\alpha}_{t,j} = 1$</span>. This normalization coefficients are then used in the backward computation as well in order to satisfy:</p><p class="math-container">\[\bar{\beta}_{t,j} = \prod_{\tau = t+1}^{N_t} \frac{1}{c_\tau} \beta_{t,j}.\]</p><p>It turns out the normalization coefficients can be used to  compute <span>$p(\mathbf{X})$</span> since at <span>$\boldsymbol\beta_{N_t} = \mathbf{1}$</span> then </p><p class="math-container">\[p(\mathbf{X}) = \sum_{j=1}^{N_s} \alpha_{N_t,j} =  \prod_{\tau = 1}^t c_\tau \sum_{j=1}^{N_s} \bar{\alpha}_{t,j}\]</p><p class="math-container">\[p(\mathbf{X}) = \prod_{\tau = 1}^{N_t} c_\tau,\]</p><p>and hence the ML cost function can be written as:</p><p class="math-container">\[\log(p(\mathbf{X})) = \sum_{\tau = 1}^{N_t} \log c_\tau.\]</p><p>Moreover this implies that the posterior probabilities  can be written as:</p><p class="math-container">\[\gamma_{t,j} = 
\frac{\alpha_{t,j} \beta_{t,j}}{p(\mathbf{X})} =
\bar{\alpha}_{t,j} \bar{\beta}_{t,j}\]</p><p>The following gradients can be derived:</p><p class="math-container">\[\frac{\partial \log (p(\mathbf{X}))}{\partial \mathbf{a} } = 
 \frac{1}{c_1} \odot \text{diag}( \mathbf{y}_1 ) \bar{\boldsymbol\beta}_1,\]</p><p class="math-container">\[\frac{\partial \log (p(\mathbf{X}))}{\partial \mathbf{A} } = 
\sum_{t=1}^{N_t-1}   
\bar{\boldsymbol\alpha}_t 
( 
\frac{1}{c_{t+1}} \odot
\text{diag}( \mathbf{y}_{t+1} ) 
\bar{\boldsymbol\beta}_{t+1} 
)^\intercal,\]</p><p class="math-container">\[\frac{\partial \log (p(\mathbf{X}))}{\partial \mathbf{y}_1 } = 
\frac{1}{c_1} \odot \text{diag}(\mathbf{a}) \bar{\boldsymbol\beta}_1\]</p><p class="math-container">\[\frac{\partial \log (p(\mathbf{X}))}{\partial \mathbf{y}_t } = 
\frac{1}{c_t} \odot 
\text{diag}(\mathbf{A}^\intercal\boldsymbol\alpha_{t-1}) \bar{\boldsymbol\beta}_t
\ \ \text{for} \ \ t = 2,\dots,N_t\]</p><p>where <span>$\odot$</span> is the element wise product (Hadamard product). These can also be expressed more compactly as:</p><p class="math-container">\[\frac{\partial \log (p(\mathbf{X}))}{\partial \mathbf{a} } = 
\text{diag}(\mathbf{a})^{-1} \boldsymbol\gamma_1 \]</p><p class="math-container">\[\frac{\partial \log (p(\mathbf{X}))}{\partial \mathbf{A} } = 
\sum_{t=1}^{N_t-1}   
\bar{\boldsymbol\alpha}_t 
( 
\mathbf{A}_t^{-1}
\bar{\boldsymbol\beta}_{t} 
)^\intercal,\]</p><p class="math-container">\[\frac{\partial \log (p(\mathbf{X}))}{\partial \mathbf{y}_t } = 
\text{diag}(\mathbf{y}_t)^{-1} \boldsymbol\gamma_t 
\ \ \text{for} \ \ t = 1,\dots,N_t\]</p><p>Notice that in terms of implementation the  former formulas are preferred as  the latter formulas are however valid only for the case where  all the elements of <span>$\mathbf{a}$</span> and <span>$\mathbf{Y}$</span> are not equal to zero and <span>$\mathbf{A}_t$</span> is invertible.</p><h2 id="Log-probabilities"><a class="docs-heading-anchor" href="#Log-probabilities">Log probabilities</a><a id="Log-probabilities-1"></a><a class="docs-heading-anchor-permalink" href="#Log-probabilities" title="Permalink"></a></h2><p>Another approach to prevent numerical overflow is to compute the forward and backward probabilities in the log-domain.</p><p class="math-container">\[\hat{\alpha}_{1,j} = \hat{a}_j + \hat{y}_{1,j}
\ \ \text{for} \ \ j =1,\dots,N_s\]</p><p class="math-container">\[\hat{\alpha}_{t,j} = \bigoplus_{i \in \mathcal{N}_{t,j}} 
\hat{\alpha}_{t-1,i} +\hat{a}_{i,j} + \hat{y}_{t,j}
\ \ \text{for} \ \ t = 2,\dots,N_t \ \ j=1,\dots,N_s,\]</p><p>where the hat symbol indicates the variable is in the log-domain, e.g. <span>$\hat{\alpha}_{i,j} = \log(\alpha_{t,j})$</span>, and <span>$\oplus(x,y) = \log(e^x+e^y)$</span>. Similarly the backward log-probabilities read:</p><p class="math-container">\[\hat{\beta}_{N_t,j} = 0
\ \ \text{for} \ \  j =1,\dots,N_s\]</p><p class="math-container">\[\hat{\beta}_{t,j} = \bigoplus_{k \in \bar{\mathcal{N}}_{t+1,j}} 
\hat{\beta}_{t+1,k} +
\hat{a}_{j,k} + \hat{y}_{t+1,k}
\ \ \text{for} \ \ t = N_t-1,\dots,1\]</p><p>The following gradients can be derived:</p><p class="math-container">\[\frac{\partial \log (p(\mathbf{X}))}{\partial \hat{\mathbf{a}} } = 
\boldsymbol\gamma_1\]</p><p class="math-container">\[\frac{\partial \log (p(\mathbf{X}))}{\partial \hat{\mathbf{A}} } = 
\frac{1}{p(\mathbf{X})}\sum_{t=1}^{N_t-1}
e^{
\hat{\alpha}_{t,i} + \hat{a}_{i,j} + \hat{\beta}_{t+1,i} + \hat{y}_{t+1,j} 
}
\ \ \text{for} \ \ i, j = 1,\dots,N_s \]</p><p class="math-container">\[\frac{\partial \log (p(\mathbf{X}))}{\partial \hat{\mathbf{y}}_t } = 
\boldsymbol\gamma_t 
\ \ \text{for} \ \ t = 1,\dots,N_t\]</p><h2 id="references"><a class="docs-heading-anchor" href="#references">References</a><a id="references-1"></a><a class="docs-heading-anchor-permalink" href="#references" title="Permalink"></a></h2><ul><li>[1] L. A. Liporace, Maximum Likelihood Estimation for Multivariate Observations of Markov Sources, IEEE Trans. Inf. Theory, 1982. </li><li>[2] S. E. Levinson, L. R. Rabiner, M. M. Sondhi, An introduction to the application of the theory of probabilistic functions of a Markov process to automatic speech recognition, Bell System Technical Journal, 1983.</li><li>[3] P. A. Devijver, Baum’s forward-backward algorithm revisited, Pattern Recognition Letters, 1985.</li></ul></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« Home</a><a class="docs-footer-nextpage" href="../2_fb/">Forward-Backward functions »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Monday 7 June 2021 16:09">Monday 7 June 2021</span>. Using Julia version 1.6.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
